{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/OoaepW+5V5E8rsra5N+o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Reverse Automatic Differentiation mit PyTorch\n","\n","[Automatic differentiation in PyTorch](https://openreview.net/pdf?id=BJJsrmfCZ)"],"metadata":{"id":"7bf02rWwtfJY"}},{"cell_type":"code","source":["import torch\n","import numpy as np"],"metadata":{"id":"PxH6pQc4sA8w","executionInfo":{"status":"ok","timestamp":1677148112548,"user_tz":-60,"elapsed":270,"user":{"displayName":"Ulf Hamster","userId":"17569698098126794188"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Mit skalaren Wert zum Verständnis"],"metadata":{"id":"q-Az1FCQu_Q0"}},{"cell_type":"code","source":["# Unsere trainierbare Variable hat einen Wert von 100.\n","a = torch.tensor(100., requires_grad=True)\n","\n","# Wir addieren einen Konstante 2 im Forward Pass\n","# Die Ableitung der \"+\" Operation ist g(x)=x\n","b = a + 2\n","\n","# Im Backward Pass g(x) ist das x bspw. ein Fehler, Loss, ... hier 123.\n","b.backward(gradient=torch.tensor(123.))\n","\n","# Der Gradient von der Variable \"a\" ist 123.\n","a.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6va0aSvmupME","executionInfo":{"status":"ok","timestamp":1677097201523,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ulf Hamster","userId":"17569698098126794188"}},"outputId":"d0c42ac6-cc6d-4f09-a2cf-dc8dcbcad2cc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(123.)"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["### Beispiel Zwei"],"metadata":{"id":"f4tu8cIDyIto"}},{"cell_type":"code","source":["# Unsere trainierbare Variable hat einen Wert von 100.\n","a = torch.tensor(100., requires_grad=True)\n","\n","# Forward Pass\n","b = a + 2.  # speichert die Ableitung g(x)=x\n","c = b * 3.  # speichert die Ableitung h(x;v)=v*x\n","\n","# Im Backward Pass g(x) ist das x bspw. ein Fehler, Loss, ... hier 123.\n","c.backward(gradient=torch.tensor(123.))\n","\n","# Der Gradient von der Variable \"a\" ist ((123.)*3)=369\n","a.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZ-sZySSyPu8","executionInfo":{"status":"ok","timestamp":1677148354766,"user_tz":-60,"elapsed":309,"user":{"displayName":"Ulf Hamster","userId":"17569698098126794188"}},"outputId":"1ef022c2-71c9-43f7-e576-d5c64ca50551"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(369.)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["### Wir basteln uns ein Loss/Fehler"],"metadata":{"id":"CXIMNCy10Vvr"}},{"cell_type":"code","source":["# Unsere trainierbare Variable hat einen Wert von 100.\n","a = torch.tensor(100., requires_grad=True) \n","\n","# Forward Pass\n","b = a + 2.\n","y_pred = b * 3.\n","\n","# Lass \"y_pred\" unser Modelloutput sein (z.B. ein Score, Logit, usw.)\n","print(f\"Model score: {y_pred}\")\n","\n","# Angenommen \"a\" soll so manipuliert werden, \n","# dass \"y_pred\" den gewünscht Score \"y_target\" entspricht\n","y_target = 290.\n","print(f\"Target score: {y_target}\")\n","\n","# Für den Fehler/Loss verwenden wir den Squared Loss\n","loss = (y_pred - y_target)**2\n","print(f\"Loss: {loss}\")\n","\n","# Backward Pass\n","loss.backward()\n","\n","# Der Gradient von der Variable \"a\"\n","a.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXmCTwkA0OtO","executionInfo":{"status":"ok","timestamp":1677148422861,"user_tz":-60,"elapsed":376,"user":{"displayName":"Ulf Hamster","userId":"17569698098126794188"}},"outputId":"e9c63ae1-cd84-444b-c824-72bcc994e524"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model score: 306.0\n","Target score: 290.0\n","Loss: 256.0\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(96.)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["### Trainiere es"],"metadata":{"id":"jB30JTVR4Lq0"}},{"cell_type":"code","source":["# Unser Zielwert\n","y_target = 290.\n","\n","# Die Lernrate für das Weight Update\n","lr = 0.01\n","\n","# Der Startwert für das trainierbare Gewicht\n","weight = 100.\n","\n","for epoch in range(10):\n","    # Forward Pass\n","    a = torch.tensor(weight, requires_grad=True) \n","    y_pred = (a + 2.) * 3\n","    # Backward Pass\n","    loss = (y_pred - y_target)**2\n","    loss.backward()\n","    # weight update (Gradient Descent)\n","    weight = weight - lr * a.grad.detach().numpy()\n","    print(f\"{loss.item():11.3f} | {weight}\")\n","\n","print(f\"weight: {weight}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQA_WybPuWiB","executionInfo":{"status":"ok","timestamp":1677149094470,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ulf Hamster","userId":"17569698098126794188"}},"outputId":"c72fac07-81e1-4fb3-8e6b-498f709311b7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["    256.000 | 99.04\n","    172.134 | 98.25280000000001\n","    115.743 | 97.607296\n","     77.826 | 97.07798272000001\n","     52.330 | 96.64394583040001\n","     35.187 | 96.28803558092801\n","     23.660 | 95.99618917636097\n","     15.909 | 95.756875124616\n","     10.697 | 95.56063760218512\n","      7.193 | 95.3997228337918\n","weight: 95.3997228337918\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ifE3ebuD5kco"},"execution_count":null,"outputs":[]}]}